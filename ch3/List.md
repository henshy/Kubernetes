# List

### 性能瓶颈问题

#### 1、k8s持续性测试，（规则）服务有较多超时

​	规则服务本身性能低下，后期会做重构。

#### 2、K8s生产环境特征工程模型组 ailab-model-group 服务异常重启

​	pod内存不够，增加内容上限。

#### 3、测试过程中Tps和延迟（ms）瓶颈

- JMeter性能瓶颈（主要为CPU）
- 容器性能拐点

### 配置问题

#### 1、一个节点断网后，中间请求还会过去

​	treafik配置问题，修改pod失效反应时间配置。

#### 2、有的节点日志丢失

​	GlusterFS节点未启动。

#### 3、测试过程中，pod分布不均匀

​        有可能全部eureka在一个节点，节点宕机会影响业务，启动反亲和配置。

#### 4、高并发情况下，会有几条请求出现500 Internal Server Error

​	Haproxy超时导致，超时时间为90s，Haproxy超时后，会告诉treafik，treafik返回该异常。

### linux bug

#### 1、压力测试后，隔了一天，其中一个节点宕机

​	内核bug（与linux与kubernetes结合后的问题），升级linux内核解决。